{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be1a80d9",
      "metadata": {},
      "source": [
        "## Quickstart: run this RAG notebook\n",
        "\n",
        "This notebook builds a **Retrieval-Augmented Generation (RAG)** pipeline over PDFs in `./docs/` (relative to this notebook).\n",
        "\n",
        "### What it does\n",
        "\n",
        "- Load all `*.pdf` files from `langchain/docs/`\n",
        "- Split pages into chunks\n",
        "- Create embeddings and persist a local **Chroma** DB\n",
        "- Retrieve relevant chunks for a question and generate an answer\n",
        "- (Optional) Start a **Gradio** chat UI\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- **Python + Jupyter** (local)\n",
        "- **Dependencies**: install from `requirements.txt`\n",
        "- **API key**: this notebook uses an **OpenAI-compatible** endpoint via OpenRouter (`base_url=\"https://openrouter.ai/api/v1\"`).\n",
        "\n",
        "### Setup\n",
        "\n",
        "1) Create and activate a virtualenv, then install deps (from repo root):\n",
        "\n",
        "```bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "2) Create a `.env` in the repo root with your API key:\n",
        "\n",
        "```text\n",
        "LLM=sk-or-v1-...\n",
        "```\n",
        "\n",
        "- The code reads `LLM` via `load_dotenv()` + `os.getenv(\"LLM\")`.\n",
        "- `.env` is gitignored; **don‚Äôt commit keys**.\n",
        "\n",
        "3) Put PDFs into `langchain/docs/`.\n",
        "\n",
        "- Note: `docs/` is gitignored (often large and/or non-redistributable).\n",
        "\n",
        "### Documents used by this notebook (current local folder)\n",
        "\n",
        "If you have the same local corpus as on this machine, `langchain/docs/` contains:\n",
        "\n",
        "...\n",
        "\n",
        "### Running\n",
        "\n",
        "- Run cells top-to-bottom.\n",
        "- The first run will build embeddings and persist the vector store under `./docs/chroma_db/`.\n",
        "  - To **re-index**, delete `langchain/docs/chroma_db/` and rerun the indexing cells.\n",
        "- Use the ‚ÄúTEST THE RAG PIPELINE‚Äù cell to try a question.\n",
        "- Run the last cell to start the Gradio UI.\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "- **`No such file or directory: ./docs`**: create `langchain/docs/` and add PDFs.\n",
        "- **`LLM environment variable not set`**: add `LLM=...` to `.env` and restart the kernel.\n",
        "- **PDF warnings** (‚ÄúIgnoring wrong pointing object ‚Ä¶‚Äù): often harmless PDF parsing noise.\n",
        "- check if RE_INDEX_CHROMA is set to false\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca5d10e",
      "metadata": {},
      "source": [
        "Front-to-back RAG implementation using LangChain + Chroma (see Quickstart above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1816bdfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "RE_INDEX_CHROMA = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed423ce4",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Complete RAG Implementation with Issue Fixes\n",
        "Addresses: API configuration, embeddings setup, database persistence, and full retrieval+generation pipeline\n",
        "\"\"\"\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PATH = './docs'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f017c109",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"clients\"\"\"\n",
        "\n",
        "LLM_KEY = os.getenv(\"LLM\")  # Ensure this is set in .env\n",
        "if not LLM_KEY:\n",
        "    raise ValueError(\"LLM environment variable not set. Check your .env file.\")\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    api_key=LLM_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",  # Remove if using OpenAI directly\n",
        "    temperature=0.7,\n",
        ")\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=LLM_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\",  # ISSUE #5 FIX: Use base_url instead of openai_api_base\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58143209",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"load and split documents\"\"\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "documents = []\n",
        "for file_path in os.listdir(PATH):\n",
        "    if file_path.endswith('.pdf'):\n",
        "        joined_path = os.path.join(PATH, file_path)\n",
        "        loader = PyPDFLoader(joined_path)\n",
        "        documents.extend(loader.load()) \n",
        "\n",
        "print(f\"Loaded {len(documents)} pages\")\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split into {len(split_docs)} chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7842edc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"create embeddings and vector store\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "if RE_INDEX_CHROMA:\n",
        "    chroma_db_path = os.path.join(PATH, \"chroma_db\")\n",
        "    db = Chroma.from_documents(\n",
        "        documents=split_docs,  # ISSUE #6 FIX: Index all chunks\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=chroma_db_path\n",
        "    )\n",
        "else:\n",
        "    chroma_db_path = os.path.join(PATH, \"chroma_db\")\n",
        "    db = Chroma(\n",
        "        embedding_function=embedding_model,\n",
        "        persist_directory=chroma_db_path\n",
        "    )\n",
        "print(f\"Vector store created with {len(split_docs)} documents\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac898ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"ü¶Æ retriever\"\"\"\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 5})  # Retrieve top 3 most relevant chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e75f17",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"rag setup\"\"\"\n",
        "\n",
        "import json\n",
        "\n",
        "rag_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a helpful assistant that answers questions based on the provided context.\n",
        "\n",
        "## CONTEXT:\n",
        "{context}\n",
        "\n",
        "## QUESTION:\n",
        "{question}\n",
        "\n",
        "## ANSWER:\n",
        "Provide a clear, concise answer based on the context above. If the context doesn't contain the answer, say so.\n",
        "Include the sources of the used context in the answer also add specific page numbers if possible. Only include the sources that are actually used in the answer.\n",
        "If there are additional readings, concepts or researchers mentioned in the context feel free to include them in the answer.\n",
        "\"\"\",\n",
        "    input_variables=[\"context\", \"question\"],\n",
        ")\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents for the prompt.\"\"\"\n",
        "    context = []\n",
        "    for retreived_doc in docs:\n",
        "        content = retreived_doc.page_content\n",
        "        source = retreived_doc.metadata[\"source\"]\n",
        "        context.append({\"source\": source, \"content\": content})\n",
        "    return json.dumps(context)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,  # Retrieve and format documents\n",
        "        \"question\": RunnablePassthrough(),  # Pass through the user question\n",
        "    }\n",
        "    | rag_prompt  # Format the prompt\n",
        "    | llm  # Send to LLM\n",
        "    | StrOutputParser()  # Parse the response as string\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8df5a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"test the pipeline\"\"\"\n",
        "\n",
        "query = \"was ist qualit√§tsmanagement?\"\n",
        "answer = rag_chain.invoke(query)\n",
        "\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c0696e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"simple out of the box frontend\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def rag_pipeline(message, history):\n",
        "    response = rag_chain.invoke(message)\n",
        "    return response\n",
        "\n",
        "demo = gr.ChatInterface(rag_pipeline)\n",
        "demo.launch()\n",
        "\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
